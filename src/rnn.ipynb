{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/DuaaTashkandi/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, Merge\n",
    "from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "maxlen = 30\n",
    "labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input = pd.read_csv(\"data_new/merged.csv\",header=None)\n",
    "input.columns = ['first', 'last','b_or_n']\n",
    "\n",
    "# remove encode \n",
    "input['first'] = input['first'].str[2:-1]\n",
    "input['last'] = input['last'].str[2:-1]\n",
    "\n",
    "input['firstlen']= [len(str(i)) for i in input['first']]\n",
    "input['lastlen'] = [len(str(i)) for i in input['last']]\n",
    "input1 = input[(input['firstlen'] >= 2) & (input['lastlen'] >= 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsts = input['first']\n",
    "lasts = input['last']\n",
    "labels = input['b_or_n']\n",
    "\n",
    "vocab = set(' '.join([str(i) for i in firsts]))\n",
    "vocab = set(' '.join([str(i) for i in lasts]))\n",
    "\n",
    "vocab.add('END')\n",
    "len_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_index = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split \n",
    "msk = np.random.rand(len(input1)) < 0.8\n",
    "\n",
    "train = input1[msk]\n",
    "test = input1[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_flag(i):\n",
    "    tmp = np.zeros(39);\n",
    "    tmp[i] = 1\n",
    "    return(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Truncating and padding training data\n",
    "train_X = []\n",
    "train_Y = []\n",
    "train_Z = []\n",
    "\n",
    "trunc_train_first = [str(i)[0:maxlen] for i in train['first']]\n",
    "trunc_train_last = [str(i)[0:maxlen] for i in train['last']]\n",
    "\n",
    "for i in trunc_train_first:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    train_X.append(tmp)\n",
    "\n",
    "for i in trunc_train_last:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    train_Y.append(tmp)\n",
    "    \n",
    "for i in train['b_or_n']:\n",
    "    if i == 1:\n",
    "        train_Z.append([1,0])\n",
    "    else:\n",
    "        train_Z.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48013, 30, 39)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(train_X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48013, 30, 39)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(train_Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48013, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(train_Z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DuaaTashkandi/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#build the model: 2 stacked LSTM\n",
    "print('Building LSTM model')\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(LSTM(512, return_sequences=True, input_shape=(maxlen,len_vocab)))\n",
    "right_branch = Sequential()\n",
    "right_branch.add(LSTM(512, return_sequences=True, input_shape=(maxlen,len_vocab)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left_branch, right_branch], mode='concat'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "\n",
    "# Softmax activation function\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Cross-entropy loss, metric is accuracy\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Truncating and padding test data\n",
    "\n",
    "test_X = []\n",
    "test_Y = []\n",
    "test_Z = []\n",
    "\n",
    "trunc_test_first = [str(i)[0:maxlen] for i in test['first']]\n",
    "trunc_test_last = [str(i)[0:maxlen] for i in test['last']]\n",
    "\n",
    "for i in trunc_test_first:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    test_X.append(tmp)\n",
    "\n",
    "for i in trunc_test_last:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    test_Y.append(tmp)\n",
    "    \n",
    "for i in test['b_or_n']:\n",
    "    if i == 1:\n",
    "        test_Z.append([1,0])\n",
    "    else:\n",
    "        test_Z.append([0,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11941, 30, 39)\n",
      "(11941, 30, 39)\n",
      "(11941, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(test_X).shape)\n",
    "print(np.asarray(test_Y).shape)\n",
    "print(np.asarray(test_Z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DuaaTashkandi/anaconda/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48013 samples, validate on 11941 samples\n",
      "Epoch 1/50\n",
      "48013/48013 [==============================] - 1474s 31ms/step - loss: 0.6589 - acc: 0.5925 - val_loss: 0.5259 - val_acc: 0.7562\n",
      "Epoch 2/50\n",
      "48013/48013 [==============================] - 1407s 29ms/step - loss: 0.4934 - acc: 0.7720 - val_loss: 0.5144 - val_acc: 0.7621\n",
      "Epoch 3/50\n",
      "48013/48013 [==============================] - 1473s 31ms/step - loss: 0.4585 - acc: 0.7941 - val_loss: 0.5232 - val_acc: 0.7586\n",
      "Epoch 4/50\n",
      "48013/48013 [==============================] - 1442s 30ms/step - loss: 0.4429 - acc: 0.8017 - val_loss: 0.4991 - val_acc: 0.7864\n",
      "Epoch 5/50\n",
      "48013/48013 [==============================] - 1495s 31ms/step - loss: 0.4213 - acc: 0.8171 - val_loss: 0.4070 - val_acc: 0.8232\n",
      "Epoch 6/50\n",
      "48013/48013 [==============================] - 1493s 31ms/step - loss: 0.3961 - acc: 0.8288 - val_loss: 0.4094 - val_acc: 0.8248\n",
      "Epoch 7/50\n",
      "48013/48013 [==============================] - 1508s 31ms/step - loss: 0.3776 - acc: 0.8404 - val_loss: 0.5977 - val_acc: 0.7334\n",
      "Epoch 8/50\n",
      "48013/48013 [==============================] - 1379s 29ms/step - loss: 0.3828 - acc: 0.8398 - val_loss: 0.4222 - val_acc: 0.8276\n",
      "Epoch 9/50\n",
      "48013/48013 [==============================] - 1381s 29ms/step - loss: 0.3641 - acc: 0.8478 - val_loss: 0.3531 - val_acc: 0.8554\n",
      "Epoch 10/50\n",
      "48013/48013 [==============================] - 1366s 28ms/step - loss: 0.3330 - acc: 0.8620 - val_loss: 0.3713 - val_acc: 0.8446\n",
      "Epoch 11/50\n",
      "48013/48013 [==============================] - 1579s 33ms/step - loss: 0.3370 - acc: 0.8623 - val_loss: 0.3468 - val_acc: 0.8591\n",
      "Epoch 12/50\n",
      "48013/48013 [==============================] - 1666s 35ms/step - loss: 0.3242 - acc: 0.8682 - val_loss: 0.3330 - val_acc: 0.8610\n",
      "Epoch 13/50\n",
      "48013/48013 [==============================] - 1506s 31ms/step - loss: 0.3162 - acc: 0.8724 - val_loss: 0.3369 - val_acc: 0.8642\n",
      "Epoch 14/50\n",
      "48013/48013 [==============================] - 1412s 29ms/step - loss: 0.3041 - acc: 0.8775 - val_loss: 0.3177 - val_acc: 0.8733\n",
      "Epoch 15/50\n",
      "48013/48013 [==============================] - 1393s 29ms/step - loss: 0.3059 - acc: 0.8768 - val_loss: 0.3424 - val_acc: 0.8572\n",
      "Epoch 16/50\n",
      "48013/48013 [==============================] - 1383s 29ms/step - loss: 0.3067 - acc: 0.8746 - val_loss: 0.3177 - val_acc: 0.8715\n",
      "Epoch 17/50\n",
      "48013/48013 [==============================] - 1376s 29ms/step - loss: 0.2874 - acc: 0.8864 - val_loss: 0.3299 - val_acc: 0.8621\n",
      "Epoch 18/50\n",
      "48013/48013 [==============================] - 1450s 30ms/step - loss: 0.3203 - acc: 0.8711 - val_loss: 0.3100 - val_acc: 0.8757\n",
      "Epoch 19/50\n",
      "48013/48013 [==============================] - 1507s 31ms/step - loss: 0.2869 - acc: 0.8865 - val_loss: 0.3051 - val_acc: 0.8822\n",
      "Epoch 20/50\n",
      "48013/48013 [==============================] - 1385s 29ms/step - loss: 0.2834 - acc: 0.8887 - val_loss: 0.3066 - val_acc: 0.8776\n",
      "Epoch 21/50\n",
      "48013/48013 [==============================] - 1374s 29ms/step - loss: 0.2846 - acc: 0.8869 - val_loss: 0.2994 - val_acc: 0.8844\n",
      "Epoch 22/50\n",
      "48013/48013 [==============================] - 1378s 29ms/step - loss: 0.2682 - acc: 0.8943 - val_loss: 0.2956 - val_acc: 0.8861\n",
      "Epoch 23/50\n",
      "48013/48013 [==============================] - 1366s 28ms/step - loss: 0.2622 - acc: 0.8983 - val_loss: 0.2951 - val_acc: 0.8843\n",
      "Epoch 24/50\n",
      "48013/48013 [==============================] - 1377s 29ms/step - loss: 0.2549 - acc: 0.9009 - val_loss: 0.2740 - val_acc: 0.8946\n",
      "Epoch 25/50\n",
      "48013/48013 [==============================] - 1373s 29ms/step - loss: 0.2451 - acc: 0.9050 - val_loss: 0.2854 - val_acc: 0.8920\n",
      "Epoch 26/50\n",
      "48013/48013 [==============================] - 1373s 29ms/step - loss: 0.2544 - acc: 0.9025 - val_loss: 0.2812 - val_acc: 0.8916\n",
      "Epoch 27/50\n",
      "48013/48013 [==============================] - 1373s 29ms/step - loss: 0.2365 - acc: 0.9096 - val_loss: 0.2833 - val_acc: 0.8916\n",
      "Epoch 28/50\n",
      "48013/48013 [==============================] - 1375s 29ms/step - loss: 0.2315 - acc: 0.9114 - val_loss: 0.2719 - val_acc: 0.8978\n",
      "Epoch 29/50\n",
      "48013/48013 [==============================] - 1378s 29ms/step - loss: 0.2280 - acc: 0.9130 - val_loss: 0.2908 - val_acc: 0.8881\n",
      "Epoch 30/50\n",
      "48013/48013 [==============================] - 1373s 29ms/step - loss: 0.2575 - acc: 0.8997 - val_loss: 0.2714 - val_acc: 0.8956\n",
      "Epoch 31/50\n",
      "48013/48013 [==============================] - 1375s 29ms/step - loss: 0.2325 - acc: 0.9121 - val_loss: 0.2734 - val_acc: 0.8952\n",
      "Epoch 32/50\n",
      "48013/48013 [==============================] - 1369s 29ms/step - loss: 0.2406 - acc: 0.9098 - val_loss: 0.2907 - val_acc: 0.8893\n",
      "Epoch 33/50\n",
      "48013/48013 [==============================] - 1369s 29ms/step - loss: 0.2200 - acc: 0.9174 - val_loss: 0.2711 - val_acc: 0.8967\n",
      "Epoch 34/50\n",
      "48013/48013 [==============================] - 1375s 29ms/step - loss: 0.2174 - acc: 0.9175 - val_loss: 0.2534 - val_acc: 0.9032\n",
      "Epoch 35/50\n",
      "48013/48013 [==============================] - 1375s 29ms/step - loss: 0.2082 - acc: 0.9226 - val_loss: 0.2710 - val_acc: 0.9029\n",
      "Epoch 36/50\n",
      "48013/48013 [==============================] - 1373s 29ms/step - loss: 0.2007 - acc: 0.9251 - val_loss: 0.2933 - val_acc: 0.8962\n",
      "Epoch 37/50\n",
      "48013/48013 [==============================] - 1376s 29ms/step - loss: 0.2033 - acc: 0.9256 - val_loss: 0.2641 - val_acc: 0.9003\n",
      "Epoch 38/50\n",
      "48013/48013 [==============================] - 1375s 29ms/step - loss: 0.1980 - acc: 0.9271 - val_loss: 0.2706 - val_acc: 0.9018\n",
      "Epoch 39/50\n",
      "48013/48013 [==============================] - 1375s 29ms/step - loss: 0.1824 - acc: 0.9331 - val_loss: 0.2745 - val_acc: 0.8991\n",
      "Epoch 40/50\n",
      "48013/48013 [==============================] - 1378s 29ms/step - loss: 0.1763 - acc: 0.9358 - val_loss: 0.2661 - val_acc: 0.9056\n",
      "Epoch 41/50\n",
      "48013/48013 [==============================] - 1380s 29ms/step - loss: 0.1752 - acc: 0.9357 - val_loss: 0.2678 - val_acc: 0.9034\n",
      "Epoch 42/50\n",
      "48013/48013 [==============================] - 1373s 29ms/step - loss: 0.1765 - acc: 0.9355 - val_loss: 0.2645 - val_acc: 0.9046\n",
      "Epoch 43/50\n",
      "48013/48013 [==============================] - 1378s 29ms/step - loss: 0.1676 - acc: 0.9381 - val_loss: 0.2742 - val_acc: 0.9008\n",
      "Epoch 44/50\n",
      "48013/48013 [==============================] - 1406s 29ms/step - loss: 0.1660 - acc: 0.9396 - val_loss: 0.2794 - val_acc: 0.8977\n",
      "Epoch 45/50\n",
      "48013/48013 [==============================] - 1380s 29ms/step - loss: 0.1781 - acc: 0.9335 - val_loss: 0.3036 - val_acc: 0.8916\n",
      "Epoch 46/50\n",
      "48013/48013 [==============================] - 1375s 29ms/step - loss: 0.2114 - acc: 0.9201 - val_loss: 0.3214 - val_acc: 0.8620\n",
      "Epoch 47/50\n",
      "48013/48013 [==============================] - 1383s 29ms/step - loss: 0.1970 - acc: 0.9245 - val_loss: 0.2628 - val_acc: 0.9011\n",
      "Epoch 48/50\n",
      "48013/48013 [==============================] - 1380s 29ms/step - loss: 0.1753 - acc: 0.9349 - val_loss: 0.2637 - val_acc: 0.9067\n",
      "Epoch 49/50\n",
      "48013/48013 [==============================] - 1377s 29ms/step - loss: 0.1546 - acc: 0.9436 - val_loss: 0.2890 - val_acc: 0.9005\n",
      "Epoch 50/50\n",
      "48013/48013 [==============================] - 1381s 29ms/step - loss: 0.1543 - acc: 0.9425 - val_loss: 0.2924 - val_acc: 0.9005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x182ea6588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "model.fit([np.asarray(train_X), np.asarray(train_Y)], train_Z, batch_size=batch_size, nb_epoch=50, validation_data=([np.asarray(test_X), np.asarray(test_Y)], test_Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11941/11941 [==============================] - 141s 12ms/step\n",
      "Test score: 0.292373455935\n",
      "Test accuracy: 0.900510844993\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate([np.asarray(test_X), np.asarray(test_Y)], test_Z)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save our model and data\n",
    "model.save_weights('model', overwrite=True)\n",
    "train.to_csv(\"train_split.csv\")\n",
    "test.to_csv(\"test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evals = model.predict([np.asarray(test_X), np.asarray(test_Y)])\n",
    "prob_m = [i[0] for i in evals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame(prob_m)\n",
    "out['first'] = test['first'].reset_index()['first']\n",
    "out['last'] = test['last'].reset_index()['last']\n",
    "out['b_or_n'] = test['b_or_n'].reset_index()['b_or_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.head(10)\n",
    "out.columns = ['prob_b','first', 'last', 'actual']\n",
    "out.head(10)\n",
    "out.to_csv(\"pred_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41819027,  0.58180976],\n",
       "       [ 0.36226737,  0.63773263],\n",
       "       [ 0.61281317,  0.38718686],\n",
       "       [ 0.85553694,  0.14446311],\n",
       "       [ 0.99840802,  0.00159197],\n",
       "       [ 0.98669618,  0.01330382]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small test. first 3 names are spanish and last 3 names are Brazilian \n",
    "first = [\"manuel\",\"luis\",\"ezequil\",\"michel\",\"luiz\",\"alvaro\"]\n",
    "last = [\"garcia\",\"sanchez\",\"gomez\",\"caetano\",\"guilherme\",\"lime\"]\n",
    "X = []\n",
    "Y = []\n",
    "trunc_first = [i[0:maxlen] for i in first]\n",
    "trunc_last = [i[0:maxlen] for i in last]\n",
    "for i in trunc_first:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "for i in trunc_last:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    Y.append(tmp)\n",
    "    \n",
    "pred = model.predict([np.asarray(X), np.asarray(Y)])\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
